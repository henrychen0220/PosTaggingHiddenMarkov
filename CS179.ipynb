{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter \"magic methods\" -- only need to be run once per kernel restart\n",
    "%load_ext autoreload\n",
    "%aimport helpers, tests\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from helpers import show_model, Dataset\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution\n",
    "from collections import namedtuple\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57340 sentences in the corpus.\n",
      "There are 45872 sentences in the training set.\n",
      "There are 11468 sentences in the testing set.\n"
     ]
    }
   ],
   "source": [
    "data = Dataset(\"tags-universal.txt\", \"brown-universal.txt\", train_test_split=0.8)\n",
    "print(\"There are {} sentences in the corpus.\".format(len(data)))\n",
    "print(\"There are {} sentences in the training set.\".format(len(data.training_set)))\n",
    "print(\"There are {} sentences in the testing set.\".format(len(data.testing_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 1161192 samples of 56057 unique words in the corpus.\n",
      "There are 928458 samples of 50536 unique words in the training set.\n",
      "There are 232734 samples of 25112 unique words in the testing set.\n",
      "There are 5521 words in the test set that are missing in the training set.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are a total of {} samples of {} unique words in the corpus.\"\n",
    "      .format(data.N, len(data.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the training set.\"\n",
    "      .format(data.training_set.N, len(data.training_set.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the testing set.\"\n",
    "      .format(data.testing_set.N, len(data.testing_set.vocab)))\n",
    "print(\"There are {} words in the test set that are missing in the training set.\"\n",
    "      .format(len(data.testing_set.vocab - data.training_set.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = (tag for i, (word, tag) in enumerate(data.training_set.stream()))\n",
    "words = (word for i, (word, tag) in enumerate(data.training_set.stream()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ADV', 'NOUN', '.', 'VERB', 'ADP', 'ADJ', 'CONJ', 'DET', 'PRT', 'NUM', 'PRON', 'X'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pair_counts(sequences_A, sequences_B):\n",
    "    dict_tag_word = defaultdict(list)\n",
    "    dict_word = defaultdict(list)\n",
    "    \n",
    "    for i, (tag, word) in  enumerate(zip(sequences_A, sequences_B)):\n",
    "        dict_tag_word[tag].append(word)\n",
    "        \n",
    "    for k in dict_tag_word.keys():\n",
    "        dict_word[k] = Counter(dict_tag_word[k])\n",
    "        \n",
    "    return dict_word\n",
    "\n",
    "pair_counts(tags,words).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = (tag for i, (word, tag) in enumerate(data.training_set.stream()))\n",
    "words = (word for i, (word, tag) in enumerate(data.training_set.stream()))\n",
    "word_counts = pair_counts(words, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FakeState = namedtuple(\"FakeState\", \"name\")\n",
    "\n",
    "class MFCTagger:\n",
    "    missing = FakeState(name=\"<MISSING>\")\n",
    "    def __init__(self, table):\n",
    "        self.table = defaultdict(lambda: MFCTagger.missing)\n",
    "        self.table.update({word: FakeState(name=tag) for word, tag in table.items()})\n",
    "        \n",
    "    def viterbi(self, seq):\n",
    "        return 0., list(enumerate([\"<start>\"] + [self.table[w] for w in seq] + [\"<end>\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfc_table = dict()\n",
    "for word, tags in word_counts.items():\n",
    "    mfc_table[word] = max(tags.keys(), key = lambda key: tags[key])\n",
    "\n",
    "mfc_model = MFCTagger(mfc_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown(sequence):\n",
    "    return [w if w in data.training_set.vocab else 'nan' for w in sequence]\n",
    "\n",
    "def simplify_decoding(X, model):\n",
    "    _, state_path = model.viterbi(replace_unknown(X))\n",
    "    return [state[1].name for state in state_path[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, Y, model):\n",
    "    correct = total_predictions = 0\n",
    "    for observations, actual_tags in zip(X, Y):\n",
    "        try:\n",
    "            most_likely_tags = simplify_decoding(observations, model)\n",
    "            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n",
    "        except:\n",
    "            pass\n",
    "        total_predictions += len(observations)\n",
    "    return correct / total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy mfc_model: 95.72%\n",
      "testing accuracy mfc_model: 93.01%\n"
     ]
    }
   ],
   "source": [
    "mfc_training_acc = accuracy(data.training_set.X, data.training_set.Y, mfc_model)\n",
    "print(\"training accuracy mfc_model: {:.2f}%\".format(100 * mfc_training_acc))\n",
    "\n",
    "mfc_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, mfc_model)\n",
    "print(\"testing accuracy mfc_model: {:.2f}%\".format(100 * mfc_testing_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_counts(sequences):\n",
    "    dictionary = defaultdict(int)\n",
    "    for i_seq in sequences:\n",
    "        dictionary[i_seq] += 1\n",
    "    return dictionary\n",
    "\n",
    "def bigram_counts(sequences):\n",
    "    dictionary = defaultdict(int)\n",
    "    bigrams = list(nltk.bigrams(tags))\n",
    "    for i_bigram in bigrams:\n",
    "        dictionary[i_bigram] += 1\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]\n",
    "tag_unigrams = unigram_counts(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]\n",
    "tag_bigrams = bigram_counts(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starting_counts(sequences):\n",
    "    start_tags = [i_seq[0] for i_seq in sequences]\n",
    "    dictionary = defaultdict(int)\n",
    "    for i_tag in start_tags:\n",
    "        dictionary[i_tag] += 1\n",
    "    return dictionary\n",
    "\n",
    "def ending_counts(sequences):\n",
    "    start_tags = [i_seq[-1] for i_seq in sequences]\n",
    "    dictionary = defaultdict(int)\n",
    "    for i_tag in start_tags:\n",
    "        dictionary[i_tag] += 1\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "tag_starts = starting_counts(data.training_set.Y)\n",
    "tag_ends = ending_counts(data.training_set.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = (tag for i, (word, tag) in enumerate(data.training_set.stream()))\n",
    "words = (word for i, (word, tag) in enumerate(data.training_set.stream()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = HiddenMarkovModel(name=\"base-hmm-tagger\")\n",
    "\n",
    "count_tag_and_word = pair_counts(tags, words)\n",
    "states = {}\n",
    "for tag, word_dict in count_tag_and_word.items():\n",
    "    p_words_given_tag_state = defaultdict(float)\n",
    "    for word in word_dict.keys():\n",
    "        p_words_given_tag_state[word] = count_tag_and_word[tag][word] / tag_unigrams[tag]\n",
    "    emission = DiscreteDistribution(dict(p_words_given_tag_state))\n",
    "    states[tag] = State(emission, name=tag)\n",
    "    \n",
    "basic_model.add_states(list(states.values()))\n",
    "\n",
    "# Adding start states\n",
    "for tag in data.training_set.tagset:\n",
    "    state = states[tag]\n",
    "    basic_model.add_transition(basic_model.start, state, tag_starts[tag]/len(data.training_set))\n",
    "\n",
    "# Adding end states\n",
    "for tag in data.training_set.tagset:\n",
    "    state = states[tag]\n",
    "    basic_model.add_transition(state, basic_model.end, tag_ends[tag]/tag_unigrams[tag])\n",
    "\n",
    "# Adding pairs\n",
    "for tag1 in data.training_set.tagset:\n",
    "    state1 = states[tag1]\n",
    "    for tag2 in data.training_set.tagset:\n",
    "        state2 = states[tag2]\n",
    "        basic_model.add_transition(state1, state2, tag_bigrams[(tag1, tag2)]/tag_unigrams[tag1])\n",
    "\n",
    "# finalize the model\n",
    "basic_model.bake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy basic hmm model: 97.52%\n",
      "testing accuracy basic hmm model: 95.94%\n"
     ]
    }
   ],
   "source": [
    "hmm_training_acc = accuracy(data.training_set.X, data.training_set.Y, basic_model)\n",
    "print(\"training accuracy basic hmm model: {:.2f}%\".format(100 * hmm_training_acc))\n",
    "\n",
    "hmm_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, basic_model)\n",
    "print(\"testing accuracy basic hmm model: {:.2f}%\".format(100 * hmm_testing_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
